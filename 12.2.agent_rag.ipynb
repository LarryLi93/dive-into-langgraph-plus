{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3444d30",
   "metadata": {},
   "source": [
    "# å†…å­˜çŸ¥è¯†æ™ºèƒ½ä½“ (Agent RAG)\n",
    "\n",
    "æœ¬ Notebook æ¼”ç¤ºå¦‚ä½•è¯»å– `files` ç›®å½•ä¸‹çš„ `.txt` æ–‡ä»¶ï¼ŒæŒ‰**ç©ºè¡Œ**åˆ‡åˆ†æ®µè½ï¼Œå¹¶æ„å»ºå‘é‡æ£€ç´¢é—®ç­”åŠ©æ‰‹ã€‚\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "1. ç¡®ä¿å·²å®‰è£…å¿…è¦ä¾èµ– (`langchain`, `openai` ç­‰)ã€‚\n",
    "2. ç¡®ä¿å½“å‰ Notebook åŒçº§ç›®å½•ä¸‹å­˜åœ¨ `files` æ–‡ä»¶å¤¹ï¼Œä¸”å…¶ä¸­åŒ…å«ä½ çš„ `.txt` æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c01ca",
   "metadata": {},
   "source": [
    "## 1. é…ç½®ç¯å¢ƒä¸æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881feeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# é…ç½® API Key (å¦‚æœªåœ¨ .env è®¾ç½®ï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹)\n",
    "if not os.getenv(\"DASHSCOPE_API_KEY\"):\n",
    "    # os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "    print(\"âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY\")\n",
    "\n",
    "os.environ[\"DASHSCOPE_BASE_URL\"] = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ç”¨äº Embedding)\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614df49f",
   "metadata": {},
   "source": [
    "## 2. å®šä¹‰ Embedding ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d67f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class DashScopeEmbeddings(Embeddings):\n",
    "    \"\"\"DashScope å…¼å®¹çš„ Embeddings å°è£…ã€‚\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"text-embedding-v4\", dimensions: int = 1024):\n",
    "        self.model = model\n",
    "        self.dimensions = dimensions\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        vectors = []\n",
    "        # ç®€å•æ‰¹å¤„ç†ï¼Œæ¯æ¬¡ 10 æ¡\n",
    "        for i in range(0, len(texts), 10):\n",
    "            chunk = texts[i : i + 10]\n",
    "            try:\n",
    "                response = client.embeddings.create(\n",
    "                    model=self.model,\n",
    "                    input=chunk,\n",
    "                    dimensions=self.dimensions,\n",
    "                )\n",
    "                vectors.extend([item.embedding for item in response.data])\n",
    "            except Exception as e:\n",
    "                print(f\"Embedding Error: {e}\")\n",
    "                vectors.extend([[0.0] * self.dimensions] * len(chunk))\n",
    "        return vectors\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        response = client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=[text],\n",
    "            dimensions=self.dimensions,\n",
    "        )\n",
    "        return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3ee20",
   "metadata": {},
   "source": [
    "## 3. è¯»å–ç°æœ‰æ–‡æ¡£å¹¶åˆ‡åˆ†\n",
    "\n",
    "æ­¤å¤„å°†è¯»å– `files/` ç›®å½•ä¸‹çš„æ‰€æœ‰ `.txt` æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰**ç©ºè¡Œ**è¿›è¡Œåˆ‡åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199cee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_txt_documents(target_dir: Path) -> list[Document]:\n",
    "    \"\"\"è¯»å–ç›®å½•ä¸‹çš„ txt æ–‡ä»¶å¹¶æŒ‰ç©ºè¡Œåˆ†å‰²ä¸º Documentã€‚\"\"\"\n",
    "\n",
    "    def split_on_blank(text: str) -> Iterable[str]:\n",
    "        # æŒ‰ç©ºè¡Œåˆ†å‰²ï¼ˆå…¼å®¹ Windows \\r\\nï¼‰\n",
    "        for block in re.split(r\"\\n\\s*\\n\", text):\n",
    "            cleaned = block.strip()\n",
    "            if cleaned:\n",
    "                yield cleaned\n",
    "\n",
    "    documents: list[Document] = []\n",
    "    \n",
    "    # æ£€æŸ¥ç›®å½•\n",
    "    if not target_dir.exists():\n",
    "        print(f\"âŒ é”™è¯¯: ç›®å½• {target_dir} ä¸å­˜åœ¨ï¼è¯·ç¡®è®¤å½“å‰è·¯å¾„ã€‚\")\n",
    "        return []\n",
    "\n",
    "    files = sorted(target_dir.glob(\"*.txt\"))\n",
    "    if not files:\n",
    "        print(f\"âŒ è­¦å‘Š: {target_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶\")\n",
    "        return []\n",
    "\n",
    "    for path in files:\n",
    "        print(f\"ğŸ“„ æ­£åœ¨è¯»å–: {path.name}\")\n",
    "        content = path.read_text(encoding=\"utf-8\")\n",
    "        for idx, part in enumerate(split_on_blank(content)):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=part,\n",
    "                    metadata={\"source\": path.name, \"chunk_id\": idx},\n",
    "                )\n",
    "            )\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54902b41",
   "metadata": {},
   "source": [
    "## 4. æ„å»ºå‘é‡åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2997c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æ­£åœ¨è¯»å–: question.txt\n",
      "\n",
      "âœ… åµŒå…¥å®Œæˆ: å…± 8 ä¸ªç‰‡æ®µã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "def build_vector_store() -> InMemoryVectorStore:\n",
    "    # æŒ‡å®š files ç›®å½• (ç›¸å¯¹äºå½“å‰ Notebook)\n",
    "    target_dir = Path.cwd() / \"files\"\n",
    "    documents = load_txt_documents(target_dir)\n",
    "\n",
    "    embeddings = DashScopeEmbeddings()\n",
    "    vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "    \n",
    "    if documents:\n",
    "        vector_store.add_documents(documents)\n",
    "        print(f\"\\nâœ… åµŒå…¥å®Œæˆ: å…± {len(documents)} ä¸ªç‰‡æ®µã€‚\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# ç«‹å³æ„å»º\n",
    "vector_store = build_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece7b09",
   "metadata": {},
   "source": [
    "## 5. åˆ›å»ºæ£€ç´¢ Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929199e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "def create_react_agent(vector_store: InMemoryVectorStore):\n",
    "    \"\"\"åˆ›å»ºå¸¦æ£€ç´¢å·¥å…·çš„ ReAct Agentã€‚\"\"\"\n",
    "\n",
    "    @tool(response_format=\"content_and_artifact\")\n",
    "    def retrieve_context(query: str):\n",
    "        \"\"\"åŸºäºå‘é‡åº“æ£€ç´¢ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚\"\"\"\n",
    "        # æ‰“å°æ—¥å¿—æ–¹ä¾¿è°ƒè¯•\n",
    "        print(f\"   ğŸ” [æ£€ç´¢ä¸­] Query: {query}\")\n",
    "        \n",
    "        retrieved = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        serialized = \"\\n\\n\".join(\n",
    "            f\"[{doc.metadata['source']}#{doc.metadata['chunk_id']}] {doc.page_content}\"\n",
    "            for doc in retrieved\n",
    "        )\n",
    "        return serialized, retrieved\n",
    "\n",
    "    return create_agent(\n",
    "        llm,\n",
    "        tools=[retrieve_context],\n",
    "        system_prompt=(\n",
    "            \"ä½ å¯ä»¥ä½¿ç”¨æ£€ç´¢å·¥å…·è·å¾—å‚è€ƒèµ„æ–™ã€‚å›ç­”æ—¶ç»“åˆæ£€ç´¢åˆ°çš„å†…å®¹ï¼Œ\"\n",
    "            \"å¦‚æœ‰å¿…è¦å¯ä»¥åœ¨ç­”æ¡ˆä¸­ç®€å•å¼•ç”¨æ¥æºæ ‡è¯†ã€‚\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd9ca9",
   "metadata": {},
   "source": [
    "## 6. æé—®æµ‹è¯•\n",
    "\n",
    "è¯·æ ¹æ®ä½ çš„ `files` ç›®å½•ä¸­çš„æ–‡æ¡£å†…å®¹ä¿®æ”¹ä¸‹é¢çš„ `query` å˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df19646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç”¨æˆ·é—®é¢˜: è€ƒå‹¤ç¼ºå¡æ€ä¹ˆå¤„ç†ï¼Ÿ\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "è€ƒå‹¤ç¼ºå¡æ€ä¹ˆå¤„ç†ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_f714d4e689774d979ecafd22)\n",
      " Call ID: call_f714d4e689774d979ecafd22\n",
      "  Args:\n",
      "    query: è€ƒå‹¤ç¼ºå¡æ€ä¹ˆå¤„ç†ï¼Ÿ\n",
      "   ğŸ” [æ£€ç´¢ä¸­] Query: è€ƒå‹¤ç¼ºå¡æ€ä¹ˆå¤„ç†ï¼Ÿ\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "[question.txt#3] é—®é¢˜:å¿˜è®°æ‰“å¡æ€ä¹ˆåŠï¼Ÿ\n",
      "ç­”æ¡ˆ:å‘˜å·¥ç¼ºå¡å¯åœ¨ç¼ºå¡å‘ç”Ÿåçš„3ä¸ªå·¥ä½œæ—¥å†…ç”³è¯·è¡¥å¡ï¼Œæ¯äººæ¯æœˆé™3æ¬¡ï¼Œè¶…é™ä¸å†å—ç†ã€‚æœªè¡¥å¡è€…ï¼Œæ¯ç¼º1æ¬¡æŒ‰ç¼ºå‹¤åŠå¤©è®¡ç®—ï¼Œæ‰£å‘åŠå¤©å·¥èµ„ã€‚\n",
      "æƒé™:\n",
      "å…³é”®è¯:å› å…¬å¤–å‡ºã€æ‰“å¡å¤„ç†\n",
      "\n",
      "[question.txt#4] é—®é¢˜:å› å…¬å¤–å‡ºæ— æ³•å›å…¬å¸æ‰“å¡æ€ä¹ˆåŠï¼Ÿ\n",
      "ç­”æ¡ˆ:å…è®¸æ‰“å¤–å‹¤å¡ï¼Œéœ€èµ°å¤–å‡ºç”³è¯·å¹¶æ³¨æ˜åŸå› ï¼›å¤–å‡ºç”³è¯·è·¯å¾„ï¼šé£ä¹¦/æ›´å¤š/å·¥ä½œå°/äººåŠ›èµ„æº&è¡Œæ”¿/å‡å‹¤/ç”³è¯·/å¤–å‡ºã€‚\n",
      "æƒé™:\n",
      "å…³é”®è¯:å› å…¬å¤–å‡º,æ— æ³•æ‰“å¡\n",
      "\n",
      "[question.txt#2] é—®é¢˜:è€ƒå‹¤æœ‰é—®é¢˜æ‰¾è°ï¼Ÿ\n",
      "ç­”æ¡ˆ:æ‰¾LarryLiã€‚\n",
      "æƒé™:\n",
      "å…³é”®è¯:è€ƒå‹¤é—®é¢˜\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å¦‚æœä½ é‡åˆ°äº†è€ƒå‹¤ç¼ºå¡çš„é—®é¢˜ï¼Œå¯ä»¥æ ¹æ®ä»¥ä¸‹æƒ…å†µå¤„ç†ï¼š\n",
      "\n",
      "1. **å¿˜è®°æ‰“å¡**ï¼šæ ¹æ®è§„å®šï¼Œå‘˜å·¥ç¼ºå¡å¯åœ¨ç¼ºå¡å‘ç”Ÿåçš„3ä¸ªå·¥ä½œæ—¥å†…ç”³è¯·è¡¥å¡ï¼Œæ¯äººæ¯æœˆé™è¡¥å¡3æ¬¡ï¼Œè¶…è¿‡é™åˆ¶å°†ä¸å†å—ç†ã€‚å¦‚æœæœªèƒ½åŠæ—¶è¡¥å¡ï¼Œæ¯ç¼ºå¡ä¸€æ¬¡å°†æŒ‰ç¼ºå‹¤åŠå¤©è®¡ç®—ï¼Œå¹¶æ‰£å‘åŠå¤©å·¥èµ„ã€‚[[3]]\n",
      "\n",
      "2. **å› å…¬å¤–å‡ºæ— æ³•å›å…¬å¸æ‰“å¡**ï¼šå¦‚æœæ˜¯å› å…¬å¤–å‡ºå¯¼è‡´æ— æ³•æ‰“å¡ï¼Œå¯ä»¥ç”³è¯·æ‰“å¤–å‹¤å¡ã€‚ä½ éœ€è¦æå‰èµ°å¤–å‡ºç”³è¯·æµç¨‹å¹¶æ³¨æ˜åŸå› ã€‚å…·ä½“æ“ä½œè·¯å¾„ä¸ºï¼šé£ä¹¦ â†’ æ›´å¤š â†’ å·¥ä½œå° â†’ äººåŠ›èµ„æº&è¡Œæ”¿ â†’ å‡å‹¤ â†’ ç”³è¯· â†’ å¤–å‡ºã€‚[[4]]\n",
      "\n",
      "å¦‚æœæœ‰å…¶ä»–è€ƒå‹¤ç›¸å…³é—®é¢˜ï¼Œå¯ä»¥è”ç³» LarryLi è¿›è¡Œå’¨è¯¢ã€‚[[2]]\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ– Agent\n",
    "agent = create_react_agent(vector_store)\n",
    "\n",
    "# === ä¿®æ”¹æ­¤å¤„çš„é—®é¢˜ ===\n",
    "query = \"è€ƒå‹¤ç¼ºå¡æ€ä¹ˆå¤„ç†ï¼Ÿ\"\n",
    "\n",
    "print(f\"ğŸš€ ç”¨æˆ·é—®é¢˜: {query}\\n\")\n",
    "\n",
    "# æµå¼è¾“å‡ºå›ç­”\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
